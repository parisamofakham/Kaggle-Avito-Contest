{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "notebookstart= time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sci-Kit Learn Models Packages\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import feature_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from math import sqrt\n",
    "\n",
    "# Light Gradient Boosting\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "# Tf-Idf\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from nltk.corpus import stopwords \n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "random.seed(2018)\n",
    "NFOLDS = 5\n",
    "SEED = 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Functions (wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define modeling function\n",
    "class SklearnWrapper(object):\n",
    "    def __init__(self, clf, seed=0, params=None, seed_bool = True):\n",
    "        if(seed_bool == True):\n",
    "            params['random_state'] = seed\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get out-of-fold predictions, the get_oof functions is used the get the out-of-fold predictions on the \n",
    "# validation set of that fold and the test set.\n",
    "# The method was used in the belowed kaggle kernel and was very helpful in different kaggle competition.\n",
    "# https://www.kaggle.com/mmueller/stacking-starter?scriptVersionId=390867\n",
    "def get_oof(clf, x_train, y, x_test):\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        print('\\nFold {}'.format(i))\n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "\n",
    "        clf.train(x_tr, y_tr)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        oof_test_skf[i, :] = clf.predict(x_test)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# text cleaning\n",
    "def cleanName(text):\n",
    "    try:\n",
    "        textProc = text.lower()\n",
    "        textProc = re.sub('[!@#$_“”¨«»®´·º½¾¿¡§£₤‘’]', '', textProc)\n",
    "        textProc = \" \".join(textProc.split())\n",
    "        return textProc\n",
    "    except: \n",
    "        return \"name error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this competition uses RMSE score\n",
    "def rmse(y, y0):\n",
    "    assert len(y) == len(y0)\n",
    "    return np.sqrt(np.mean(np.power((y - y0), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# numerical to categorical preparation\n",
    "# previouly in the data cleaing, -999 was used to deal with NA and other errors\n",
    "def rep999(x):\n",
    "    try:\n",
    "        if x == -999:\n",
    "            return -1\n",
    "        else:\n",
    "            return x\n",
    "    except:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### the only difference between lgb and lgbcat is some numerical features are transformed into categorical feature in lgbcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training = pd.read_csv('train.csv', index_col = \"item_id\",parse_dates = [\"activation_date\"])\n",
    "traindex = training.index\n",
    "# new features were extracted and put in to another file\n",
    "newtrain = pd.read_csv('new_train_features.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Numerical to categorical transformation\n",
    "newtrain=newtrain.apply(rep999)\n",
    "newtrain[\"perform_white_analysis\"]=pd.cut(newtrain['perform_white_analysis'], range(-5, 100, 5))\n",
    "newtrain[\"perform_black_analysis\"]=pd.cut(newtrain['perform_black_analysis'], range(-5, 100, 5))\n",
    "newtrain[\"image_size\"]=pd.cut(newtrain['image_size'], range(-100, 5000, 100))\n",
    "newtrain[\"average_pixel_width\"]=pd.cut(newtrain['average_pixel_width'], range(-2, 50, 1))\n",
    "newtrain[\"get_blurrness_score\"]=pd.cut(newtrain['get_blurrness_score'], range(-200,10000, 200))\n",
    "newtrain[[\"average_red\",\"average_green\",\"average_blue\"]]=newtrain[[\"average_red\",\"average_green\",\"average_blue\"]].apply(lambda x: round(100*x,2))\n",
    "newtrain[\"average_red\"]=pd.cut(newtrain['average_red'], range(-110, 110, 10))\n",
    "newtrain[\"average_green\"]=pd.cut(newtrain['average_green'], range(-110, 110, 10))\n",
    "newtrain[\"average_blue\"]=pd.cut(newtrain['average_blue'], range(-110, 110, 10))\n",
    "newtrain[\"rural\"]=pd.cut(newtrain['rural'], range(-10, 100, 10))\n",
    "newtrain[\"city_population\"]=newtrain['city_population'].apply(lambda x: np.log(x+2))\n",
    "newtrain[\"reg_Population\"]=newtrain['reg_Population'].apply(lambda x: np.log(x+2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature combination\n",
    "training = pd.concat([training.reset_index(drop=True), newtrain], axis=1)\n",
    "training.index=traindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading test data\n",
    "testing = pd.read_csv('test.csv', index_col = \"item_id\",parse_dates = [\"activation_date\"])\n",
    "testdex = testing.index\n",
    "# new features were extracted and put in to another file\n",
    "newtest = pd.read_csv('new_test_features.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Numerical to categorical transformation\n",
    "newtest=newtest.apply(rep999)\n",
    "newtest[\"perform_white_analysis\"]=pd.cut(newtest['perform_white_analysis'], range(-5, 100, 5))\n",
    "newtest[\"perform_black_analysis\"]=pd.cut(newtest['perform_black_analysis'], range(-5, 100, 5))\n",
    "newtest[\"image_size\"]=pd.cut(newtest['image_size'], range(-100, 5000, 100))\n",
    "newtest[\"average_pixel_width\"]=pd.cut(newtest['average_pixel_width'], range(-2, 50, 1))\n",
    "newtest[\"get_blurrness_score\"]=pd.cut(newtest['get_blurrness_score'], range(-200,10000, 200))\n",
    "newtest[[\"average_red\",\"average_green\",\"average_blue\"]]=newtest[[\"average_red\",\"average_green\",\"average_blue\"]].apply(lambda x: round(100*x,2))\n",
    "newtest[\"average_red\"]=pd.cut(newtest['average_red'], range(-110, 110, 10))\n",
    "newtest[\"average_green\"]=pd.cut(newtest['average_green'], range(-110, 110, 10))\n",
    "newtest[\"average_blue\"]=pd.cut(newtest['average_blue'], range(-110, 110, 10))\n",
    "newtest[\"rural\"]=pd.cut(newtest['rural'], range(-10, 100, 10))\n",
    "newtest[\"city_population\"]=newtest['city_population'].apply(lambda x: np.log(x+2))\n",
    "newtest[\"reg_Population\"]=newtest['reg_Population'].apply(lambda x: np.log(x+2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature combination\n",
    "testing = pd.concat([testing.reset_index(drop=True), newtest], axis=1)\n",
    "testing.index=testdex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data shape check\n",
    "ntrain = training.shape[0]\n",
    "ntest = testing.shape[0]\n",
    "y = training.deal_probability.copy()\n",
    "training.drop(\"deal_probability\",axis=1, inplace=True)\n",
    "print('Train shape: {} Rows, {} Columns'.format(*training.shape))\n",
    "print('Test shape: {} Rows, {} Columns'.format(*testing.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# k-fold modeling\n",
    "kf = KFold(ntrain, n_folds=NFOLDS, shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training and testing concatenation\n",
    "dfs=[training,testing]\n",
    "df = pd.concat(dfs,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Validation Index and Remove Dead Variables\n",
    "training_index = df.loc[df.activation_date<=pd.to_datetime('2017-04-07')].index\n",
    "validation_index = df.loc[df.activation_date>=pd.to_datetime('2017-04-08')].index\n",
    "df.drop([\"activation_date\",\"image\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# catagorical variables encoding\n",
    "print(\"\\nEncode Variables\")\n",
    "categorical1 = [\"perform_white_analysis\",\"perform_black_analysis\",\"image_size\",\"average_pixel_width\",\"get_blurrness_score\",\"average_red\",\"average_green\",\"average_blue\",\"rural\"]\n",
    "categorical2 = [\"descsentiment\",\"titlesentiment\",\"user_id\",\"region\",\"city\",\"parent_category_name\",\"category_name\",\"user_type\",\"image_top_1\",\"param_1\",\"param_2\",\"param_3\",\"reg_Time_zone\"]\n",
    "categorical=categorical1+categorical2\n",
    "print(\"Encoding :\",categorical)\n",
    "\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for col in categorical1:\n",
    "    df[col] = lbl.fit_transform(df[col].astype(str))\n",
    "for col in categorical2:\n",
    "    df[col].fillna('Unknown')\n",
    "    df[col] = lbl.fit_transform(df[col].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tfidf setting\n",
    "print(\"Term Frequency Inverse Document Frequency Stage\")\n",
    "russian_stop = set(stopwords.words('russian'))\n",
    "\n",
    "tfidf_para = {\n",
    "    \"stop_words\": russian_stop,\n",
    "    \"analyzer\": 'word',\n",
    "    \"token_pattern\": r'\\w{1,}',\n",
    "    \"sublinear_tf\": True,\n",
    "    \"dtype\": np.float32,\n",
    "    \"norm\": 'l2',\n",
    "    \"smooth_idf\":False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# text cleaning\n",
    "textfeats = [\"description\", \"title\"]\n",
    "df['desc_punc'] = df['description'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "\n",
    "df['title'] = df['title'].apply(lambda x: cleanName(x))\n",
    "df[\"description\"]   = df[\"description\"].apply(lambda x: cleanName(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# text vectorizer\n",
    "def get_col(col_name): return lambda x: x[col_name]\n",
    "\n",
    "vectorizer = FeatureUnion([\n",
    "        ('description',TfidfVectorizer(\n",
    "            ngram_range=(1, 2),\n",
    "            max_features=17000,\n",
    "            **tfidf_para,\n",
    "            preprocessor=get_col('description'))),\n",
    "        ('title',CountVectorizer(\n",
    "            ngram_range=(1, 2),\n",
    "            stop_words = russian_stop,\n",
    "            #max_features=7000,\n",
    "            preprocessor=get_col('title')))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_vect=time.time()\n",
    "vectorizer.fit(df.to_dict('records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ready_df = vectorizer.transform(df.to_dict('records'))\n",
    "tfvocab = vectorizer.get_feature_names()\n",
    "print(\"Vectorization Runtime: %0.2f Minutes\"%((time.time() - start_vect)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop Text Cols\n",
    "textfeats = [\"description\", \"title\"]\n",
    "df.drop(textfeats, axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "ridge_params = {'alpha':30.0, 'fit_intercept':True, 'normalize':False, 'copy_X':True,\n",
    "                'max_iter':None, 'tol':0.001, 'solver':'auto', 'random_state':SEED}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge = SklearnWrapper(clf=Ridge, seed = SEED, params = ridge_params)\n",
    "ridge_oof_train, ridge_oof_test = get_oof(ridge, ready_df[:ntrain], y, ready_df[ntrain:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ridge Results\n",
    "# We will use ridge in our lgb to improve performance\n",
    "rms = sqrt(mean_squared_error(y, ridge_oof_train))\n",
    "print('Ridge OOF RMSE: {}'.format(rms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating new feature\n",
    "ridge_preds = np.concatenate([ridge_oof_train, ridge_oof_test])\n",
    "df['ridge_preds'] = ridge_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine Dense Features with Sparse Text Features\n",
    "X = hstack([csr_matrix(df.loc[traindex,:].values),ready_df[0:traindex.shape[0]]])\n",
    "testing = hstack([csr_matrix(df.loc[testdex,:].values),ready_df[traindex.shape[0]:]])\n",
    "tfvocab = df.columns.tolist() + tfvocab\n",
    "print(\"Feature Names Length: \",len(tfvocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "lgbm_params =  {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    # 'max_depth': 15,\n",
    "    'num_leaves': 270,\n",
    "    'feature_fraction': 0.5,\n",
    "    'bagging_fraction': 0.75,\n",
    "    'bagging_freq': 2,\n",
    "    'learning_rate': 0.0175,\n",
    "    'verbose': 0\n",
    "}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LGB Dataset\n",
    "lgtrain = lgb.Dataset(X, y,\n",
    "                feature_name=tfvocab,\n",
    "                categorical_feature = categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# running lgb\n",
    "# the process would take 3-4 hours\n",
    "lgb_clf = lgb.train(\n",
    "    lgbm_params,\n",
    "    lgtrain,\n",
    "    num_boost_round=2000,\n",
    "    verbose_eval=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lgpred = lgb_clf.predict(testing) \n",
    "lgbmodel = pd.DataFrame(lgpred,columns=[\"deal_probability\"],index=testdex)\n",
    "\n",
    "# The probability should be between 0 and 1\n",
    "lgbmodel['deal_probability'].clip(0.0, 1.0, inplace=True)\n",
    "lgbmodel.to_csv(\"lgb.csv\",index=True,header=True)\n",
    "\n",
    "print(\"Notebook Runtime: %0.2f Minutes\"%((time.time() - notebookstart)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The output was use to do model blending and was not submitted to kaggle directly\n",
    "##### It's Validation RMSE is 0.01 higher than original lgb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
